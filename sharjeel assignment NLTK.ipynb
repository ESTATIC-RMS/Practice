{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2234d10-e86a-4877-aae2-cba4d61ede7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WELCOME TO THE WORLD OF PYTHON PROGRAMMING!\n"
     ]
    }
   ],
   "source": [
    "# uppercasing\n",
    "text = \"Welcome to the world of Python Programming!\"\n",
    "text = text.upper()  \n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c4db965-cf93-4fec-9f67-77e1080112fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'the', 'world', 'of', 'python', 'programming', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\MUHAMMAD\n",
      "[nltk_data]     SHOAIB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Tokenization\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Welcome to the world of Python Programming!\"\n",
    "text = text.lower()\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "002586d2-1234-4084-821a-9cddf05b8010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\MUHAMMAD\n",
      "[nltk_data]     SHOAIB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\MUHAMMAD\n",
      "[nltk_data]     SHOAIB\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'powerful', 'language', 'natural', 'language', 'processing', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#Removing Stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Python is a powerful language for natural language processing.\"\n",
    "tokens = word_tokenize(text)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "480ed07a-e68e-40b1-97d3-e1af0090927f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow Python Java  C are amazing languages\n"
     ]
    }
   ],
   "source": [
    "#Removing Punctuation\n",
    "import string\n",
    "\n",
    "text = \"Wow! Python, Java & C++ are amazing languages.\"\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d133455-cb99-413e-b4ce-a1a7c8da6bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\MUHAMMAD\n",
      "[nltk_data]     SHOAIB\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['play', 'play', 'play']\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"playing\", \"played\", \"plays\"]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "print(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2786909-6743-4c97-918d-7ce0198e267a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['play', 'play', 'play', 'play']\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"playing\", \"played\", \"playful\", \"plays\"]\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eb942c7-81dd-44b2-9637-cb772d1294ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is great for NLP.\n"
     ]
    }
   ],
   "source": [
    "#Removing Extra Space\n",
    "text = \"   Python   is    great for   NLP.   \"\n",
    "text = ' '.join(text.split())\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ac287ec-6535-4f9d-97dd-063daf4209e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow Python is awesome fast  and good Language\n"
     ]
    }
   ],
   "source": [
    "#Removing Special Characters\n",
    "import re\n",
    "\n",
    "text = \"Wow!! Python is @awesome, #fast & and good Language.\"\n",
    "text = re.sub(r'[^\\w\\s]', '', text)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c18eefa-7668-48cc-810d-0262ed4bd969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She bought  pens and  notebooks for the exam.\n"
     ]
    }
   ],
   "source": [
    "#Removing Numbers\n",
    "import re\n",
    "\n",
    "text = \"She bought 12 pens and 7 notebooks for the exam.\"\n",
    "text = re.sub(r'\\d+', '', text)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6fd2b98-0b95-49ed-9d57-35165401adbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\MUHAMMAD\n",
      "[nltk_data]     SHOAIB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\MUHAMMAD SHOAIB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\MUHAMMAD\n",
      "[nltk_data]     SHOAIB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\MUHAMMAD\n",
      "[nltk_data]     SHOAIB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b7e524e-1910-454b-b9fe-129598accb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Python', 'NNP'), ('makes', 'VBZ'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('easy', 'JJ'), ('and', 'CC'), ('fun', 'NN'), ('!', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\MUHAMMAD SHOAIB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#Part of speech (Pos) Tagging\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the required resource\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tokens = word_tokenize(\"Python makes natural language processing easy and fun!\")\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df7f2255-19d9-49f1-a178-a66ee1fca3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   message\n",
      "0   Python\n",
      "1     cool\n",
      "2       is\n",
      "3     cool\n",
      "4  awesome\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MUHAMMAD SHOAIB\\AppData\\Local\\Temp\\ipykernel_17740\\2713762344.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"message\"].fillna(\"cool\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#Handling missing values\n",
    "import pandas as pd\n",
    "\n",
    "data = {\"message\": [\"Python\", None, \"is\", None, \"awesome\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df[\"message\"].fillna(\"cool\", inplace=True)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a18f26b9-915b-4fac-b717-74067805628e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 1 0 0 1 1 0]\n",
      " [1 1 0 0 0 1 1 1 1 1]]\n",
      "['and' 'data' 'for' 'great' 'is' 'learning' 'machine' 'python' 'science'\n",
      " 'uses']\n"
     ]
    }
   ],
   "source": [
    "#vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"Python is great for data science.\", \"Data science uses Python and machine learning.\"]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ed2c34d-9f94-4171-b33d-969aeed9f243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naïve façade\n"
     ]
    }
   ],
   "source": [
    "#text Encoding\n",
    "text = \"naïve façade\"\n",
    "text = text.encode('utf-8').decode('utf-8')\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd1338b2-4608-49db-9793-d1e1b7b75f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Machien learnig is a brnch of Arificial Inteligence\n",
      "Corrected Text: Machine learning is a branch of Artificial Intelligence\n"
     ]
    }
   ],
   "source": [
    "#Spelling Correction\n",
    "from textblob import TextBlob\n",
    "\n",
    "text = \"Machien learnig is a brnch of Arificial Inteligence\"\n",
    "blob = TextBlob(text)\n",
    "corrected_text = blob.correct()\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Corrected Text:\", corrected_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06172dcd-09d1-431c-96ad-e24633dfa0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cliche resume\n"
     ]
    }
   ],
   "source": [
    "#Normalization\n",
    "import unicodedata\n",
    "\n",
    "text = \"Cliché résumé\"\n",
    "text = unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df74e698-6a59-48f6-90f0-6aef5c09df94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', 'Python']\n"
     ]
    }
   ],
   "source": [
    "#Handling Whitespace Tokens\n",
    "tokens = [\"Hello\", \" \", \"World\", \"  \", \"\", \"Python\", \" \"]\n",
    "tokens = [token for token in tokens if token.strip()]\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e20d4d0-a105-4df6-a2c3-1918f63df837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b8ce4b-686b-4875-9ee8-5965e2e7998a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
